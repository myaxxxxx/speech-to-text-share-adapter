






<h2 align="center">
Learning Efficient Multilingual Parameter-Sharing Adapters for Machine Translation
</h2>

<p align="center">
  <!-- <img src="https://img.shields.io/badge/EMNLP-2023-brightgreen"> -->
  <!-- <under review><img src="http://img.shields.io/badge/Paper-PDF-red.svg"></a> -->
  <img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg">
  <img src="https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?e&logo=PyTorch&logoColor=white">
</p>

<p align="center">
 Codes for our paper Multilingual Parameter-Sharing Adapters (Under review). 
</p>


### Installation

1. Create a conda environment with Pytorch:

```
conda create --name teast_env python=3.9
source activate teast_env
```

2. Install fairseq

```bash
git clone https://github.com/pytorch/fairseq
cd fairseq
pip install --editable ./
# or install fairseq from the current dir
pip install --editable ./

# Next
python setup.py build develop

# on MacOS:
# CFLAGS="-stdlib=libc++" pip install --editable ./

# to install the latest stable release (0.10.x)
# pip install fairseq
```

3. Other operations

Due to the version compatibility of packages, you also need to reinstall the following packages：

```bash
# numpy np.float error 
pip install numpy==1.23.5

# generation error: sacrebleu import error TOKENIZER 
pip install sacrebleu==1.5.1
```

### Datasets

Due to the anonymous Github upload space limitation, we currently only upload the test set. After the double-blind review period, we will publish the Fairseq Databin file on Google Drive.

1.  Download [Vocabulary](https://deltalm.blob.core.windows.net/deltalm/dict.txt) and [ Sentencepiece-model](https://deltalm.blob.core.windows.net/deltalm/spm.model) of deltalm and you need to tokenize raw data to spm data. 
2.  Preprocess spm data. 

WMT preprocess

```

ROOT=/data/translation/ted_8_diverse_spm
DICT=$ROOT/dict.txt
DATA_BIN=/data/translation/ted_8_diverse_o2m
RAW_DATA=$ROOT

# for lang in "bos" "mar" "hin" "mkd" "ell" "bul" "fra" "kor";

for item in "bos" "mar" "hin" "mkd" "ell" "bul" "fra" "kor";

do 
echo $item

fairseq-preprocess  \
    --trainpref $RAW_DATA/train.$item-eng \
    --testpref $RAW_DATA/valid.$item-eng \
    --validpref $RAW_DATA/test.$item-eng \
    --source-lang eng  --target-lang $item \
    --destdir $DATA_BIN \
    --srcdict $DICT \
    --tgtdict $DICT \
    --workers 40

done
```

### Training

In order to reproduce the results of our paper,  run the following commands:

Diverse DATASETS M2O

```


lang_pairs="bos-eng,mar-eng,hin-eng,mkd-eng,ell-eng,bul-eng,fra-eng,kor-eng"


path_2_data=/path/to/data
lang_list=${path_2_data}/lang_list_diverse.txt
SAVE_DIR=/path/to/save
USER_DIR=adapter_deltalm

PRETRAINED_MODEL=/path/to/pretrain_model/deltalm-base.pt
mkdir -vp $SAVE_DIR

export CUDA_VISIBLE_DEVICES=0,1,2,3

python train.py $path_2_data \
    --arch deltalm_base  \
    --user-dir $USER_DIR \
    --encoder-normalize-before --decoder-normalize-before \
    --pretrained-deltalm-checkpoint $PRETRAINED_MODEL \
    --share-all-embeddings \
    --max-source-positions 512 --max-target-positions 512 \
    --task translation_multi_simple_epoch \
    --sampling-method "temperature" \
    --sampling-temperature 5 \
    --decoder-langtok \
    --encoder-langtok "src" \
    --lang-dict "$lang_list" \
    --lang-pairs "$lang_pairs" \
    --criterion label_smoothed_cross_entropy \
    --label-smoothing 0.1 \
    --optimizer adam --adam-betas '(0.9, 0.98)' \
    --lr-scheduler inverse_sqrt \
    --lr 8e-5 \
    --warmup-init-lr 1e-07 \
    --stop-min-lr 1e-09 \
    --warmup-updates 6000 \
    --max-update 400000 \
    --max-epoch 50 \
    --max-tokens 8000 \
    --update-freq 1 \
    --seed 1 \
    --skip-invalid-size-inputs-valid-test \
    --tensorboard-logdir $SAVE_DIR/tensorboard \
    --save-dir $SAVE_DIR/checkpoints \
    --keep-last-epochs 50 \
    --fp16 --adapters-bottle 128

# for o2m, you can set --encoder-langtok "tgt" for original m2o data-bin file.
```

### Evaluation

Diverse M2O

```
checkpoint_path=$SAVE_DIR/checkpoints

OUTPUT_DIR=$checkpoint_path/$i

mkdir -p $OUTPUT_DIR

model=${checkpoint_path}/checkpoint$i.pt
echo "model: ${model}"
for tgt in eng; do
    for src in bos mar hin mkd ell bul fra kor; do
        python generate.py $path_2_data \
            --path $model \
            --user-dir $USER_DIR \
            --task translation_multi_simple_epoch \
            --lang-dict "$lang_list" \
            --lang-pairs "$lang_pairs" \
            --gen-subset test \
            --source-lang $src \
            --target-lang $tgt \
            --encoder-langtok "src" \
            --scoring sacrebleu \
            --remove-bpe 'sentencepiece'\
            --batch-size 96 \
            --decoder-langtok > $OUTPUT_DIR/test_${src}_${tgt}.txt 2>&1

    done
done

# for o2m, you can set --encoder-langtok "tgt" for original m2o data-bin file.
```

### NAS-inspired

For training convenience, we add a regular adapter to each layer of the multilingual pre-training model and store parameters weight_A and weight_B in the regular adapter. After the first stage of training, we count the weight_A and weight_B to get the weights_list. we then perform the following calculations in the second stage:

```

encoder_list = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1,1,1 ]

x1 = self.adapters[index](x)
x2 = self.share_adapter(x)

weights_softmax = nn.Softmax(dim=-1)
weights_adapter = weights_softmax(torch.cat((self.adapters[index].weight_A, self.adapters[index].weight_B)))
x = (1- share_list[index]) * x1 * weights_adapter[0] + share_list[index] * x2 * weights_adapter[1]

## decoder_list is similar to encoder
```

After getting the optimal architecture, we design the corresponding specific model and migrate the parameters.

### More results

#### BLEU scores of parameter-sharing adapters and NAS-inspired approach on the TED Diverse dataset
|  | Method | bos | mar | hin | mkd | ell | bul | fra | kor | Avg. |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| M2O | Parameter-sharing adapters | 39.40 | 22.78 | 27.90 | 39.16 | 38.13 | 40.32 | 42.23 | 20.84 | 33.84 |
|  | NAS-inspired approach | 38.88 | 23.52 | 28.63 | 40.38 | 39.73 | 41.55 | 43.30 | 22.03 | 34.75 |
| O2M | Parameter-sharing adapters | 19.03 | 4.21 | 9.24 | 21.60 | 21.83 | 29.19 | 33.95 | 3.59 | 17.83 |
|  | NAS-inspired approach | 19.79 | 4.21 | 9.24 | 21.60 | 21.83 | 29.19 | 34.70 | 3.59 | 18.01 |


---

#### BLEU scores of parameter-sharing adapters and NAS-inspired approach on the TED Related dataset
|  | Method | aze | bel | glg | slk | tur | rus | por | ces | Avg. |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| M2O | Parameter-sharing adapters | 21.04 | 31.05 | 40.57 | 34.68 | 30.50 | 27.85 | 47.04 | 30.55 | 32.91 |
|  | NAS-inspired approach | 21.87 | 31.72 | 41.26 | 34.34 | 30.30 | 28.35 | 47.19 | 35.62 | 33.83 |
| O2M | Parameter-sharing adapters | 9.50 | 18.78 | 30.92 | 22.24 | 15.72 | 20.23 | 38.58 | 23.01 | 22.37 |
|  | NAS-inspired approach | 10.20 | 18.94 | 31.32 | 31.70 | 23.35 | 8.89 | 39.07 | 24.13 | 23.45 |


---

#### BLEU scores of parameter-sharing adapters and NAS-inspired approach on the WMT dataset
|  | Method | Fr | De | Zh | Et | Ro | Tr | Avg. |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| M2O | Parameter-sharing adapters | 31.63 | 29.73 | 15.32 | 21.13 | 37.91 | 23.94 | 26.61 |
|  | NAS-inspired approach | 31.71 | 29.73 | 15.27 | 21.71 | 36.26 | 22.46 | 26.18 |
| O2M | Parameter-sharing adapters | 29.72 | 23.68 | 12.74 | 13.15 | 25.14 | 13.83 | 19.71 |
|  | NAS-inspired approach | 29.50 | 23.44 | 12.51 | 12.96 | 24.78 | 13.72 | 19.48 |


### NAS-inspired approach training loss curve
![](training_loss2.png#id=UEGkS&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
### Acknowledgement

We refer to the code of [Deltalm](https://github.com/microsoft/unilm/tree/master/deltalm) and [LSSD](https://github.com/OrangeInSouth/LSSD). Thanks for their great contributions!
